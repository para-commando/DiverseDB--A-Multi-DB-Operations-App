# Cassandra storage config YAML 

 
# The name of the cluster. This is mainly used to prevent machines in
# one logical cluster from joining another.
cluster_name: 'Test Cluster'

# The number of tokens randomly assigned to this node in a cluster that uses virtual nodes (vnodes). This setting is evaluated in relation to the num_tokens set on other nodes in the cluster. If this node's num_tokens value is higher than the values on other nodes, the vnode logic assigns this node a larger proportion of data relative to other nodes. In general, if all nodes have equal hardware capability, each one should have the same num_tokens value . The recommended value is 256. If this property is commented out (#num_tokens), Cassandra uses 1 (equivalent to #num_tokens : 1) for legacy compatibility and assigns tokens using the initial_token property.
# If this cluster is not using vnodes, comment out num_tokens or set num_tokens: 1 and use initial_token.
num_tokens: 256

# this is node-specific
# The listen_address in Cassandra's configuration specifies the IP address or hostname the node uses to connect to other nodes. Key considerations:
# Single-Node: For single-node setups, comment out this property or use the default (localhost).
# Multi-Node: In multi-node setups, set listen_address to the node's IP or hostname.
# Multi-Network: In complex setups with multiple networks or data centers, configure listen_address accordingly.
# Caution: Never set listen_address to 0.0.0.0; it's always incorrect.
# Exclusive Setting: Do not set both listen_address and listen_interface on the same node; choose one or the other.

# listen_address: 172.17.0.2

# This approach abstracts away the need to specify an IP address directly and simply tells Cassandra to use the network interface named eth0.
# It can be more convenient in situations where the network interface name (eth0) is stable and less likely to change, especially in static or well-configured environments.
# It's a way to delegate the responsibility of choosing the IP address to the system's network configuration.
# If your network setup is relatively stable and you want a more abstract way to specify the network interface without worrying about IP address changes, then using listen_interface can be more convenient.
# Note that this doesn't support ip-aliasing, IP aliasing is a technique used in computer networking to assign multiple IP addresses to a single network interface (NIC) on a computer or network device. This technique allows a single NIC to have multiple IP addresses associated with it, each serving different purposes or used for specific network-related tasks. 

listen_interface: eth0

# If an interface has an ipv4 and an ipv6 address, Cassandra uses the first ipv4 address by default. Set this property to true to configure Cassandra to use the first ipv6 address.
# since ipv6 is more improved version when compared to ipv4 hence set as true
listen_interface_prefer_ipv6: true

# while the commit log and CDC log directories should be within the same filesystem for management reasons, it's also a common practice to consider placing the commit log on a separate storage device or partition to improve write performance and data durability, especially in production environments with high write workloads.
# commit log.  when running on magnetic HDD, this should be a
# separate spindle than the data directories.
# If not set, the default directory is $CASSANDRA_HOME/data/commitlog.
commitlog_directory: /var/lib/cassandra/commitlog

# the data_file_directories setting is used to specify the directories where Cassandra stores its data files, including SSTables (Sorted String Table files). SSTables are the primary on-disk storage format for data in Cassandra.

# Here's an explanation of the data_file_directories setting:

# data_file_directories: This is a list of one or more directory paths where Cassandra will store its data files. You can specify multiple directories by separating them with commas. Cassandra will evenly distribute the data across these directories, helping to balance the I/O load.
# Having multiple data directories can help distribute the I/O load across multiple devices or disks, improving both read and write performance and increasing fault tolerance. It can also be useful when you have different types of storage devices (e.g., SSDs and HDDs) and want to allocate specific keyspaces or column families to different storage types based on their performance requirements.
# If not set, the default directory is $CASSANDRA_HOME/data/data.
data_file_directories: /var/lib/cassandra/data

# saved caches
# saved_caches_directory setting specifies the directory where Cassandra stores its saved caches. Saved caches are used to speed up certain read operations by caching frequently accessed data structures, such as the row key cache and the counter cache.
# The saved_caches_directory is used for storing various types of caches, including:

# Row Key Cache: This cache stores frequently accessed row keys, improving read performance by reducing the number of disk seeks when retrieving data.

# Counter Cache: The counter cache stores frequently accessed counter values (used in Cassandra's counter column type).

# Key Cache: The key cache stores frequently accessed partition keys, further optimizing read performance.

# Bloom Filter: While not exactly a cache, Cassandra's bloom filters are also persisted to disk in this directory. Bloom filters are used to reduce disk I/O by quickly determining whether data for a particular key may exist on disk.

# Having a dedicated directory for saved caches helps in managing cache data across node restarts and ensures that the cache data is persistent. This can lead to improved read performance, especially for frequently accessed data.

# It's important to allocate sufficient space for the saved_caches_directory, as the size of saved caches can grow over time depending on the workload and usage patterns. Additionally, you should monitor and maintain the saved cache directory to prevent it from consuming too much disk space.
# If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.
saved_caches_directory: /var/lib/cassandra/saved_caches

# determines how the system should respond when a commit disk failure occurs. A commit disk failure refers to a situation where the disk that stores the commit log, which is essential for ensuring data durability, encounters an issue

# policy for commit disk failures:
# die: shut down gossip and Thrift and kill the JVM, so the node can be replaced.
# stop: shut down gossip and Thrift, leaving the node effectively dead, but
#       can still be inspected via JMX.
# stop_commit: shutdown the commit log, letting writes collect but
#              continuing to service reads, as in pre-2.0.5 Cassandra
# ignore: ignore fatal errors and let the batches fail
# More conservative policies like "die" or "stop" prioritize data safety and require manual intervention to bring the node back online. Less conservative policies like "ignore" prioritize availability but can risk data loss or corruption.
commit_failure_policy : stop

# The strategy for optimizing disk read
# Possible values are:
# ssd (for solid state disks, the default)
# spinning (for spinning disks)
disk_optimization_strategy: ssd

# This setting defines the action Cassandra should take in response to a disk failure, specifically when one of the disks where data is stored becomes unavailable or fails.
# Disk failures can occur due to hardware issues, disk corruption, or other factors. When a disk failure is detected, Cassandra will respond according to the specified policy.
# policy for data disk failures:
# die: shut down gossip and client transports and kill the JVM for any fs errors or
#      single-sstable errors, so the node can be replaced.
# stop_paranoid: shut down gossip and client transports even for single-sstable errors,
#                kill the JVM for errors during startup.
# stop: shut down gossip and client transports, leaving the node effectively dead, but
#       can still be inspected via JMX, kill the JVM for errors during startup.
# best_effort: stop using the failed disk and respond to requests based on
#              remaining available sstables.  This means you WILL see obsolete
#              data at CL.ONE!
# ignore: ignore fatal errors and let requests fail, as in pre-1.2 Cassandra
disk_failure_policy: stop

# determines how Cassandra nodes perceive and locate each other within the cluster based on their network topology. The endpoint_snitch is a critical configuration option for cluster management, as it helps nodes make decisions about which other nodes to communicate with and how to route requests effectively.
# Cassandra provides several built-in endpoint snitches, each designed for different network topologies and use cases. Some common built-in snitches include:

# SimpleSnitch: A basic snitch that doesn't take network topology into account. It assumes that all nodes are in the same data center and rack.

# GossipingPropertyFileSnitch: This snitch uses a configuration file (cassandra-rackdc.properties) to specify the data center and rack assignments for each node. It's a good choice for small to medium-sized clusters with simple topologies.

# EC2Snitch: Designed for clusters deployed on Amazon EC2, this snitch uses EC2's metadata service to automatically determine the data center and rack information for nodes
 # PropertyFileSnitch
# Determines proximity by rack and datacenter, which are explicitly configured in cassandra-topology.properties file.
# Ec2MultiRegionSnitch
# Uses the public IP as the broadcast_address to allow cross-region connectivity. This means you must also set seed addresses to the public IP and open the storage_port or ssl_storage_port on the public IP firewall. For intra-region traffic, Cassandra switches to the private IP after establishing a connection.
# RackInferringSnitch:
# Proximity is determined by rack and datacenter, which are assumed to correspond to the 3rd and 2nd octet of each node's IP address, respectively. Best used as an example for writing a custom snitch class (unless this happens to match your deployment conventions).

# GoogleCloudSnitch:
# Use for Cassandra deployments on Google Cloud Platform across one or more regions. The region is treated as a datacenter and the availability zones are treated as racks within the datacenter. All communication occurs over private IP addresses within the same logical network.
# CloudstackSnitch
# Use the CloudstackSnitch for Apache Cloudstack environments.


endpoint_snitch: org.apache.cassandra.locator.GossipingPropertyFileSnitch
#  This setting is used for client communication with the Cassandra cluster. It defines the IP address or hostname that clients (such as application servers) should use to connect to the Cassandra cluster. The rpc_address is where the Cassandra Query Language (CQL) clients send their requests for reading or writing data.
# rpc_address : localhost


# The listen address for client connections. Interface must correspond to a single address, IP aliasing is not supported
rpc_interface: eth1


rpc_interface_prefer_ipv6: true

# In Apache Cassandra, a seed node is a special type of node within a Cassandra cluster that is used as an initial contact point for other nodes in the cluster to join or discover the cluster. Seed nodes play a crucial role in the node discovery process and are used during the bootstrapping of new nodes in the cluster. Here's what seed nodes are used for and how they work:

# Node Discovery: When a new node is added to a Cassandra cluster or when a node restarts, it needs to discover and establish initial connections with other nodes in the cluster to become a part of the cluster. Seed nodes act as the entry points for this discovery process.

# Gossip Protocol: Cassandra uses a protocol called the Gossip Protocol for node-to-node communication and cluster metadata distribution. Seed nodes are used to initiate the Gossip Protocol. When a new node starts, it contacts one or more seed nodes to get information about the cluster, such as the IP addresses of other nodes.

# Cluster Membership: Seed nodes maintain information about the cluster's topology and the addresses of other nodes. New nodes use seed nodes to learn about the existing nodes in the cluster and establish connections with them.

# Scalability: Seed nodes help maintain the scalability and stability of the cluster by providing a controlled way for new nodes to join. Without seed nodes, each node might need to know the addresses of all other nodes in the cluster, which can become impractical in large clusters.

# It's important to note that while seed nodes are used during the initial node discovery and cluster formation, they do not have any special status within the cluster once it is up and running. All nodes in a Cassandra cluster communicate with each other using the Gossip Protocol, and seed nodes are just the starting points for that communication.

# Typically, you would designate a few nodes in your Cassandra cluster as seed nodes by configuring them in the Cassandra configuration file (cassandra.yaml). These seed nodes should be stable and well-connected nodes within your cluster.

# In summary, seed nodes in Cassandra are initial contact points that help new or restarting nodes discover and join the cluster by providing information about the cluster's topology and other nodes.


seed_provider :

    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          # seeds is actually a comma-delimited list of addresses.
          # Ex: "<ip1>,<ip2>,<ip3>" 
          # this is node-specific
          - seeds: "172.17.0.2,172.17.0.3"

# UDFs (user defined functions) are disabled by default.
# As of Cassandra 3.0 there is a sandbox in place that should prevent execution of evil code.
enable_user_defined_functions: false

# Enables scripted UDFs (JavaScript UDFs).
# Java UDFs are always enabled, if enable_user_defined_functions is true.
# Enable this option to be able to use UDFs with "language javascript" or any custom JSR-223 provider.
# This option has no effect, if enable_user_defined_functions is false.
enable_scripted_user_defined_functions: false



