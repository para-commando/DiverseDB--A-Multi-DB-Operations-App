# Cassandra storage config YAML 

 
# The name of the cluster. This is mainly used to prevent machines in
# one logical cluster from joining another.
cluster_name: 'Test Cluster'

# The number of tokens randomly assigned to this node in a cluster that uses virtual nodes (vnodes). This setting is evaluated in relation to the num_tokens set on other nodes in the cluster. If this node's num_tokens value is higher than the values on other nodes, the vnode logic assigns this node a larger proportion of data relative to other nodes. In general, if all nodes have equal hardware capability, each one should have the same num_tokens value . The recommended value is 256. If this property is commented out (#num_tokens), Cassandra uses 1 (equivalent to #num_tokens : 1) for legacy compatibility and assigns tokens using the initial_token property.
# If this cluster is not using vnodes, comment out num_tokens or set num_tokens: 1 and use initial_token.
num_tokens: 256

# this is node-specific
# The listen_address in Cassandra's configuration specifies the IP address or hostname the node uses to connect to other nodes. Key considerations:
# Single-Node: For single-node setups, comment out this property or use the default (localhost).
# Multi-Node: In multi-node setups, set listen_address to the node's IP or hostname.
# Multi-Network: In complex setups with multiple networks or data centers, configure listen_address accordingly.
# Caution: Never set listen_address to 0.0.0.0; it's always incorrect.
# Exclusive Setting: Do not set both listen_address and listen_interface on the same node; choose one or the other.

# listen_address: 172.17.0.2

# This approach abstracts away the need to specify an IP address directly and simply tells Cassandra to use the network interface named eth0.
# It can be more convenient in situations where the network interface name (eth0) is stable and less likely to change, especially in static or well-configured environments.
# It's a way to delegate the responsibility of choosing the IP address to the system's network configuration.
# If your network setup is relatively stable and you want a more abstract way to specify the network interface without worrying about IP address changes, then using listen_interface can be more convenient.
# Note that this doesn't support ip-aliasing, IP aliasing is a technique used in computer networking to assign multiple IP addresses to a single network interface (NIC) on a computer or network device. This technique allows a single NIC to have multiple IP addresses associated with it, each serving different purposes or used for specific network-related tasks. 

listen_interface: eth0

# If an interface has an ipv4 and an ipv6 address, Cassandra uses the first ipv4 address by default. Set this property to true to configure Cassandra to use the first ipv6 address.
# since ipv6 is more improved version when compared to ipv4 hence set as true
listen_interface_prefer_ipv6: true

# while the commit log and CDC log directories should be within the same filesystem for management reasons, it's also a common practice to consider placing the commit log on a separate storage device or partition to improve write performance and data durability, especially in production environments with high write workloads.
# commit log.  when running on magnetic HDD, this should be a
# separate spindle than the data directories.
# If not set, the default directory is $CASSANDRA_HOME/data/commitlog.
commitlog_directory: /var/lib/cassandra/commitlog

# the data_file_directories setting is used to specify the directories where Cassandra stores its data files, including SSTables (Sorted String Table files). SSTables are the primary on-disk storage format for data in Cassandra.

# Here's an explanation of the data_file_directories setting:

# data_file_directories: This is a list of one or more directory paths where Cassandra will store its data files. You can specify multiple directories by separating them with commas. Cassandra will evenly distribute the data across these directories, helping to balance the I/O load.
# Having multiple data directories can help distribute the I/O load across multiple devices or disks, improving both read and write performance and increasing fault tolerance. It can also be useful when you have different types of storage devices (e.g., SSDs and HDDs) and want to allocate specific keyspaces or column families to different storage types based on their performance requirements.
# If not set, the default directory is $CASSANDRA_HOME/data/data.
data_file_directories: /var/lib/cassandra/data

# saved caches
# saved_caches_directory setting specifies the directory where Cassandra stores its saved caches. Saved caches are used to speed up certain read operations by caching frequently accessed data structures, such as the row key cache and the counter cache.
# The saved_caches_directory is used for storing various types of caches, including:

# Row Key Cache: This cache stores frequently accessed row keys, improving read performance by reducing the number of disk seeks when retrieving data.

# Counter Cache: The counter cache stores frequently accessed counter values (used in Cassandra's counter column type).

# Key Cache: The key cache stores frequently accessed partition keys, further optimizing read performance.

# Bloom Filter: While not exactly a cache, Cassandra's bloom filters are also persisted to disk in this directory. Bloom filters are used to reduce disk I/O by quickly determining whether data for a particular key may exist on disk.

# Having a dedicated directory for saved caches helps in managing cache data across node restarts and ensures that the cache data is persistent. This can lead to improved read performance, especially for frequently accessed data.

# It's important to allocate sufficient space for the saved_caches_directory, as the size of saved caches can grow over time depending on the workload and usage patterns. Additionally, you should monitor and maintain the saved cache directory to prevent it from consuming too much disk space.
# If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.
saved_caches_directory: /var/lib/cassandra/saved_caches

# determines how the system should respond when a commit disk failure occurs. A commit disk failure refers to a situation where the disk that stores the commit log, which is essential for ensuring data durability, encounters an issue

# policy for commit disk failures:
# die: shut down gossip and Thrift and kill the JVM, so the node can be replaced.
# stop: shut down gossip and Thrift, leaving the node effectively dead, but
#       can still be inspected via JMX.
# stop_commit: shutdown the commit log, letting writes collect but
#              continuing to service reads, as in pre-2.0.5 Cassandra
# ignore: ignore fatal errors and let the batches fail
# More conservative policies like "die" or "stop" prioritize data safety and require manual intervention to bring the node back online. Less conservative policies like "ignore" prioritize availability but can risk data loss or corruption.
commit_failure_policy : stop

# The strategy for optimizing disk read
# Possible values are:
# ssd (for solid state disks, the default)
# spinning (for spinning disks)
disk_optimization_strategy: ssd

# This setting defines the action Cassandra should take in response to a disk failure, specifically when one of the disks where data is stored becomes unavailable or fails.
# Disk failures can occur due to hardware issues, disk corruption, or other factors. When a disk failure is detected, Cassandra will respond according to the specified policy.
# policy for data disk failures:
# die: shut down gossip and client transports and kill the JVM for any fs errors or
#      single-sstable errors, so the node can be replaced.
# stop_paranoid: shut down gossip and client transports even for single-sstable errors,
#                kill the JVM for errors during startup.
# stop: shut down gossip and client transports, leaving the node effectively dead, but
#       can still be inspected via JMX, kill the JVM for errors during startup.
# best_effort: stop using the failed disk and respond to requests based on
#              remaining available sstables.  This means you WILL see obsolete
#              data at CL.ONE!
# ignore: ignore fatal errors and let requests fail, as in pre-1.2 Cassandra
disk_failure_policy: stop

# determines how Cassandra nodes perceive and locate each other within the cluster based on their network topology. The endpoint_snitch is a critical configuration option for cluster management, as it helps nodes make decisions about which other nodes to communicate with and how to route requests effectively.
# Cassandra provides several built-in endpoint snitches, each designed for different network topologies and use cases. Some common built-in snitches include:

# SimpleSnitch: A basic snitch that doesn't take network topology into account. It assumes that all nodes are in the same data center and rack.

# GossipingPropertyFileSnitch: This snitch uses a configuration file (cassandra-rackdc.properties) to specify the data center and rack assignments for each node. It's a good choice for small to medium-sized clusters with simple topologies.

# EC2Snitch: Designed for clusters deployed on Amazon EC2, this snitch uses EC2's metadata service to automatically determine the data center and rack information for nodes
 # PropertyFileSnitch
# Determines proximity by rack and datacenter, which are explicitly configured in cassandra-topology.properties file.
# Ec2MultiRegionSnitch
# Uses the public IP as the broadcast_address to allow cross-region connectivity. This means you must also set seed addresses to the public IP and open the storage_port or ssl_storage_port on the public IP firewall. For intra-region traffic, Cassandra switches to the private IP after establishing a connection.
# RackInferringSnitch:
# Proximity is determined by rack and datacenter, which are assumed to correspond to the 3rd and 2nd octet of each node's IP address, respectively. Best used as an example for writing a custom snitch class (unless this happens to match your deployment conventions).

# GoogleCloudSnitch:
# Use for Cassandra deployments on Google Cloud Platform across one or more regions. The region is treated as a datacenter and the availability zones are treated as racks within the datacenter. All communication occurs over private IP addresses within the same logical network.
# CloudstackSnitch
# Use the CloudstackSnitch for Apache Cloudstack environments.


endpoint_snitch: org.apache.cassandra.locator.GossipingPropertyFileSnitch
#  This setting is used for client communication with the Cassandra cluster. It defines the IP address or hostname that clients (such as application servers) should use to connect to the Cassandra cluster. The rpc_address is where the Cassandra Query Language (CQL) clients send their requests for reading or writing data.
# rpc_address : localhost


# The listen address for client connections. Interface must correspond to a single address, IP aliasing is not supported
rpc_interface: eth1


rpc_interface_prefer_ipv6: true

# In Apache Cassandra, a seed node is a special type of node within a Cassandra cluster that is used as an initial contact point for other nodes in the cluster to join or discover the cluster. Seed nodes play a crucial role in the node discovery process and are used during the bootstrapping of new nodes in the cluster. Here's what seed nodes are used for and how they work:

# Node Discovery: When a new node is added to a Cassandra cluster or when a node restarts, it needs to discover and establish initial connections with other nodes in the cluster to become a part of the cluster. Seed nodes act as the entry points for this discovery process.

# Gossip Protocol: Cassandra uses a protocol called the Gossip Protocol for node-to-node communication and cluster metadata distribution. Seed nodes are used to initiate the Gossip Protocol. When a new node starts, it contacts one or more seed nodes to get information about the cluster, such as the IP addresses of other nodes.

# Cluster Membership: Seed nodes maintain information about the cluster's topology and the addresses of other nodes. New nodes use seed nodes to learn about the existing nodes in the cluster and establish connections with them.

# Scalability: Seed nodes help maintain the scalability and stability of the cluster by providing a controlled way for new nodes to join. Without seed nodes, each node might need to know the addresses of all other nodes in the cluster, which can become impractical in large clusters.

# It's important to note that while seed nodes are used during the initial node discovery and cluster formation, they do not have any special status within the cluster once it is up and running. All nodes in a Cassandra cluster communicate with each other using the Gossip Protocol, and seed nodes are just the starting points for that communication.

# Typically, you would designate a few nodes in your Cassandra cluster as seed nodes by configuring them in the Cassandra configuration file (cassandra.yaml). These seed nodes should be stable and well-connected nodes within your cluster.

# In summary, seed nodes in Cassandra are initial contact points that help new or restarting nodes discover and join the cluster by providing information about the cluster's topology and other nodes.


seed_provider :

    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          # seeds is actually a comma-delimited list of addresses.
          # Ex: "<ip1>,<ip2>,<ip3>" 
          # this is node-specific
          - seeds: "172.17.0.2,172.17.0.3"

# UDFs (user defined functions) are disabled by default.
# As of Cassandra 3.0 there is a sandbox in place that should prevent execution of evil code.
enable_user_defined_functions: false

# Enables scripted UDFs (JavaScript UDFs).
# Java UDFs are always enabled, if enable_user_defined_functions is true.
# Enable this option to be able to use UDFs with "language javascript" or any custom JSR-223 provider.
# This option has no effect, if enable_user_defined_functions is false.
enable_scripted_user_defined_functions: false

# The setting is used to throttle the rate at which data compaction occurs. Compaction can be an I/O-intensive operation, and without rate limiting, it can potentially consume excessive system resources, causing performance issues.
# this determines the rate at which compaction processes are allowed to consume system resources, specifically measured in megabytes per second (MB/s). Compaction is the process of merging and compacting data files (SSTables) to optimize storage and improve read performance You can set compaction_throughput_mb_per_sec to a specific value in MB/s that represents the maximum rate at which compaction processes are allowed to run
# The optimal value for compaction_throughput_mb_per_sec depends on your hardware, workload, and the overall cluster configuration. You might need to experiment with different values to find the right balance. In general:

# For spinning hard drives (HDDs), a lower value is often appropriate, such as 16-64 MB/s, to avoid excessive I/O contention.
# For solid-state drives (SSDs) or high-performance storage, you can set a higher value, such as 128-256 MB/s, to take advantage of the faster I/O capabilities.
#  Setting the value to 0 disables compaction throttling.
compaction_throughput_mb_per_sec: 16

# Cassandra logs a warning when compacting partitions larger than the set value.
compaction_large_partition_warning_threshold_mb: 100

# The amount of on-heap memory allocated for memtables. Cassandra uses the total of this amount and the value of memtable_offheap_space_in_mb to set a threshold for automatic memtable flush.
memtable_heap_space_in_mb : 500
# Sets the total amount of off-heap memory allocated for memtables. Cassandra uses the total of this amount and the value of memtable_heap_space_in_mb to set a threshold for automatic memtable flush.
memtable_offheap_space_in_mb: 500

# Workloads with more data than can fit in memory encounter a bottleneck in fetching data from disk during reads. Setting concurrent_reads to (16 × number_of_drives) allows operations to queue low enough in the stack so that the OS and drives can reorder them. The default setting applies to both logical volume managed (LVM) and RAID drives
concurrent_reads: 32
# Writes in Cassandra are rarely I/O bound, so the ideal number of concurrent writes depends on the number of CPU cores on the node. The recommended value is 8 × number_of_cpu_cores
concurrent_writes: 32

#  A counter is a numeric value that can be incremented or decremented atomically across multiple columns within a single row. It's a data type used to keep track of counts or metrics in a distributed database like Cassandra.
#  This setting determines the number of simultaneous counter writes that Cassandra can handle concurrently on a single replica (node) without blocking other write operations. Counter writes read the current values before incrementing and writing them back. The recommended value is (16 × number_of_drives) .
concurrent_counter_writes: 32

# configuration file controls the level of parallelism for batch log writes. Batch log writes are used to ensure that multiple write operations (updates, inserts, deletes) are applied atomically, even if they affect multiple rows or columns across different tables.
# This setting determines the number of simultaneous batch log write operations that Cassandra can handle concurrently on a single replica (node) without blocking other write operations

concurrent_batchlog_writes: 32 

#  Limit on the number of concurrent materialized view writes. Set this to the lesser of concurrent reads or concurrent writes, because there is a read involved in each materialized view write.
concurrent_materialized_view_writes: 32

# Backs up data updated since the last snapshot was taken. When enabled, Cassandra creates a hard link to each SSTable flushed or streamed locally in a backups subdirectory of the keyspace data. Removing these links is the operator's responsibility.
incremental_backups: true

# Enables or disables taking a snapshot before each compaction. A snapshot is useful to back up data when there is a data format change. Be careful using this option: Cassandra does not clean up older snapshots automatically.
snapshot_before_compaction: true

# Adjusts the sensitivity of the failure detector on an exponential scale. Generally this setting does not need adjusting
phi_convict_threshold: 8

# commitlog_sync: periodic:

# When set to periodic, Cassandra will periodically sync (flush) data to the commit log based on the commitlog_sync_period_in_ms configuration parameter.
# The commitlog_sync_period_in_ms specifies the time interval in milliseconds between sync operations.
# This mode is less I/O-intensive than batch mode, as it doesn't immediately sync every write operation but rather batches them and syncs periodically. This can lead to a slight risk of data loss in case of a crash if data hasn't been synced yet.
# in this writes may be acked immediately
# and the CommitLog is simply synced every commitlog_sync_period_in_ms
# milliseconds. 
# commitlog_sync: periodic
# commitlog_sync_period_in_ms: 10000
# commitlog_sync: batch:

# When set to batch, Cassandra will immediately sync (flush) data to the commit log after each write operation. This means that every write operation is guaranteed to be safely written to the commit log before it is considered complete.
# batch mode provides stronger durability guarantees and minimizes the risk of data loss in case of a crash because data is immediately written to disk.
# However, it can be more I/O-intensive compared to periodic mode, potentially affecting write performance, especially on systems with high write throughput.
# commitlog_sync may be either "periodic" or "batch." 
# 
# When in batch mode, Cassandra won't ack writes until the commit log
# has been fsynced to disk.  It will wait
# commitlog_sync_batch_window_in_ms milliseconds between fsyncs.
# This window should be kept short because the writer threads will
# be unable to do extra work while waiting.  (You may need to increase
# concurrent_writes for the same reason.)
commitlog_sync: batch
commitlog_sync_batch_window_in_ms: 2

# controls the segment size on disk, where the commit logs are stored and once that size is reached then new segment is created.

commitlog_segment_size_in_mb: 32

# Max mutation size is also configurable via max_mutation_size_in_kb setting in
# cassandra.yaml. The default is half the size commitlog_segment_size_in_mb * 1024.
#
# NOTE: If max_mutation_size_in_kb is set explicitly then commitlog_segment_size_in_mb must
# be set to at least twice the size of max_mutation_size_in_kb / 1024
# formula is max_mutation_size_in_kb = (commitlog_segment_size_in_mb/2)*2
max_mutation_size_in_kb: 16384

# warning is been set to trigger when batch size is greater than or equal to 12 mb ( 12288 kb)
batch_size_warn_threshold_in_kb: 12288

# When commitlog_compression is enabled, Cassandra uses compressed commit log segments instead of memory-mapped segments. This can help reduce disk usage and improve performance, especially when dealing with large amounts of data. However, it is important to note that enabling compression can also increase CPU usage, as the data must be compressed and decompressed as it is written and read from disk
# "Snappy" refers to the Snappy compression algorithm, which is a fast and efficient compression algorithm. Snappy is known for its speed and low CPU usage, making it a good choice for systems where compression and decompression need to be performed quickly with minimal impact on system performance
commitlog_compression: Snappy


# captures change data capture, Change Data Capture is a mechanism that allows you to capture and track changes made to data within Cassandra tables. It is particularly useful in scenarios where you need to replicate data changes to external systems, perform data synchronization, or enable real-time analytics and auditing.
cdc_enabled: true

# This setting specifies the directory where the raw CDC log files are stored. 
cdc_raw_directory: /var/lib/cassandra/cdc_raw


# Cassandra provides a CDC feature that allows you to capture change data for tables in a cluster. These changes can be consumed by external applications or used for auditing and data analysis purposes. The cdc_total_space_in_mb property is used to specify the maximum amount of disk space, in megabytes, that can be consumed by the CDC log files.

# When CDC is enabled for a table in Cassandra, change data is written to CDC log files on disk. These log files capture the details of the changes, such as inserts, updates, and deletes made to the table. The CDC log files are stored in a directory defined by the cdc_raw_directory property.

# The cdc_total_space_in_mb property ensures that the CDC log files do not consume excessive disk space and helps manage the overall storage requirements for change data. If the total space occupied by the CDC log files exceeds the value set for this property, Cassandra stops writing change data until enough log files are removed or cleared to free up space.
cdc_total_space_in_mb: 500

# When the cdc_raw limit is hit and the CDCCompactor is either running behind or experiencing backpressure, this interval is checked to see if any new space for cdc-tracked tables has been made available.
cdc_free_space_check_interval_ms: 250 


#  allows you to set a limit on the total disk space that the commit log directory can use. When this limit is reached, Cassandra will start recycling or removing older commit log segments to free up space for new write operations. This limit helps prevent the commit log from consuming all available disk space
commitlog_total_space_in_mb : 40

# control how many simultaneous compaction tasks can be executed at once, In Apache Cassandra, compaction is not inherently a blocking operation. However, during the compaction process, different compaction strategies and configurations can impact the behavior of reads and writes in the system.

# By default, Cassandra utilizes a concurrent compaction model, where multiple compaction tasks can be executed concurrently. This means that compactions can occur simultaneously with read and write operations without blocking them entirely.

# However, it's important to consider that compaction operations can consume system resources such as CPU, I/O, and disk bandwidth. If the compaction workload or system resources are heavily utilized, it may have an impact on the overall performance of read and write operations during compaction.
concurrent_compactors: 2

#  determines the size threshold, in megabytes (MB), that triggers preemptive opening of SSTable files during read operations. This setting is related to the optimization of read performance in Cassandra.

# Here's an explanation of sstable_preemptive_open_interval_in_mb:

# SSTable Preemptive Open: Cassandra stores data in SSTables (Sorted String Tables) on disk. When a read operation is requested, Cassandra typically opens the SSTable file associated with the read request, reads the necessary data, and serves the request.

# Preemptive Opening: Preemptive opening refers to the practice of opening and caching SSTable files in memory before they are explicitly requested by read operations. This can improve read performance because it reduces the latency associated with opening files from disk when a read request arrives.

# sstable_preemptive_open_interval_in_mb: This setting defines the size threshold for SSTable files. When a read operation is requested, Cassandra checks the size of the SSTable file. If the file's size exceeds the value specified in sstable_preemptive_open_interval_in_mb, Cassandra may preemptively open and cache the SSTable in memory, anticipating future read requests.
sstable_preemptive_open_interval_in_mb: 50

# The method Cassandra uses to allocate and manage memtable memory. See Off-heap memtables in Cassandra 2.1. In releases 3.2.0 and 3.2.1, the only option that works is: heap-buffers (On heap NIO (non-blocking I/O) buffers).
memtable_allocation_type: heap_buffers

#  Cassandra adds memtable_heap_space_in_mb to memtable_offheap_space_in_mb and multiplies the total by memtable_cleanup_threshold to get a space amount in MB. When the total amount of memory used by all non-flushing memtables exceeds this amount, Cassandra flushes the largest memtable to disk.
# For example, consider a node where the total of memtable_heap_space_in_mb and memtable_offheap_space_in_mb is 1000, and memtable_cleanup_threshold is 0.50. The memtable_cleanup amount is 500MB. This node has two memtables: Memtable A (150MB) and Memtable B (350MB). When either memtable increases, the total space they use exceeds 500MB and Cassandra flushes the Memtable B to disk.
# A larger value for memtable_cleanup_threshold means larger flushes, less frequent flushes and potentially less compaction activity, but also less concurrent flush activity, which can make it difficult to keep your disks saturated under heavy write load
memtable_cleanup_threshold: 0.5
#  As read operations are performed on SSTables, Cassandra checks the file cache first to see if the requested data is already in memory. If found, the data is retrieved from the cache, avoiding disk I/O.
file_cache_size_in_mb : 100
#  Indicates whether Cassandra allocates allocate on-heap or off-heap memory when the SSTable buffer pool is exhausted (when the buffer pool has exceeded the maximum memory file_cache_size_in_mb), beyond this amount, Cassandra stops caching buffers, but allocates on request.
buffer_pool_use_heap_if_exhausted: true

# configure the number of concurrent writer threads that can perform memtable flushes. These writer threads work in parallel to flush data from multiple memtables, which can improve write performance by reducing the time it takes to write data to disk.  setting it too high may result in excessive contention for system resources and may not necessarily lead to better performance.

memtable_flush_writers: 2

#  In Cassandra, data is organized into columns within rows. To efficiently look up and retrieve specific columns, Cassandra uses on-disk column indexes. These indexes contain information about the location of columns within SSTables, allowing for fast data retrieval.

# Column Index Size: column_index_size_in_kb specifies the maximum size, in kilobytes (KB), for the on-disk column indexes associated with SSTables. In other words, it determines how much disk space can be used for storing column indexes.

# Default Value: The default value for column_index_size_in_kb may vary depending on the Cassandra version and configuration. It is usually set to a moderate value that balances the need for efficient indexing with disk space considerations.

# Tuning Considerations:

# Increasing column_index_size_in_kb allows for larger and potentially more detailed column indexes, which can improve read performance by reducing the need to perform extensive scans of SSTable data.
# Decreasing this setting conserves disk space but may result in less detailed indexes and slower read performance, especially for queries that need to locate specific columns within large SSTables.
column_index_size_in_kb : 64

# Fixed memory pool size in MB for SSTable index summaries. If the memory usage of all index summaries exceeds this limit, any SSTables with low read rates shrink their index summaries to meet this limit. This is a best-effort process. In extreme conditions, Cassandra may use more than this amount of memory.
# The primary purpose of an index summary is to provide a high-level overview of the column index data contained within an SSTable. Rather than storing the complete index information for every column, which can be space-intensive, an index summary contains a subset of key information
# leave it as empty for default values
index_summary_capacity_in_mb:

# Resizing Index Summaries: Over time, as data is written to an SSTable, the structure and distribution of data within the SSTable may change. For example, column values may be added, updated, or deleted. To maintain optimal query performance, index summaries need to be periodically updated to reflect these changes.

# Resize Interval: index_summary_resize_interval_in_minutes specifies the time interval, in minutes, at which Cassandra checks if index summaries need to be resized or rebuilt for an SSTable. If an SSTable has experienced significant changes since the last index summary was built, resizing may be necessary to ensure the summary remains accurate and efficient.

# Default Value: The default value for index_summary_resize_interval_in_minutes is typically set to a reasonable interval, such as 60 minutes. This means that every hour, Cassandra checks SSTables for changes and may initiate index summary resizing if needed.

# Performance Impact: Resizing index summaries can consume CPU and I/O resources, as it involves scanning the SSTable to update the summary. Therefore, the frequency of resizing should be balanced with the system's resource availability and query performance requirements.
index_summary_resize_interval_in_minutes: 60

# This setting controls the maximum outbound network throughput (in megabits per second, Mbps) for streaming file transfers on a node.
# Streaming is often used during operations like bootstrap or repair. These operations involve transferring data between nodes.
# Limiting the throughput helps prevent excessive network saturation during streaming, which could negatively impact other network activities like client (RPC) requests.
# The default value is 200 Mbps.
stream_throughput_outbound_megabits_per_sec: 200

# Similar to the previous setting, this one throttles the outbound network throughput for streaming file transfers, but it specifically applies to data transfers between datacenters.
# It also affects network stream traffic configured with stream_throughput_outbound_megabits_per_sec.
# Controlling the inter-datacenter streaming throughput helps manage the bandwidth usage between different datacenter locations
inter_dc_stream_throughput_outbound_megabits_per_sec: 200

# When set to true, this option forces the operating system to periodically flush dirty buffers ( 
# Dirty buffers are areas of memory that contain data that has been changed but not yet written to disk. This data is typically stored in dirty buffers until there is enough data to write to disk efficiently.

# Dirty buffers can improve performance by reducing the number of times that the operating system has to write to disk. However, if the operating system crashes or there is a power outage, the data in the dirty buffers may be lost. )to disk at the interval defined by trickle_fsync_interval_in_kb.
# trickle_fsync is a configuration parameter in the Cassandra cassandra.yaml file that controls how Cassandra flushes data to disk. When set to true, Cassandra will force the operating system to flush the dirty buffers at a set interval (trickle_fsync_interval_in_kb) when doing sequential writing. This can help to improve read latencies by preventing sudden dirty buffer flushing from impacting them.

# trickle_fsync is recommended to use on SSDs, but not on HDDs. This is because SSDs have much faster flush times than HDDs, so the impact on read latencies is less significant.
trickle_fsync: true

# The default value of trickle_fsync_interval_in_kb is 10240. This means that Cassandra will flush the dirty buffers to disk every 10240 kilobytes of data written when trickle_fsync is enabled.

trickle_fsync_interval_in_kb: 10240

