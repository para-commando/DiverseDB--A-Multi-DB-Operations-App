# Cassandra storage config YAML 

 
# The name of the cluster. This is mainly used to prevent machines in
# one logical cluster from joining another.
cluster_name: 'Test Cluster'

# The number of tokens randomly assigned to this node in a cluster that uses virtual nodes (vnodes). This setting is evaluated in relation to the num_tokens set on other nodes in the cluster. If this node's num_tokens value is higher than the values on other nodes, the vnode logic assigns this node a larger proportion of data relative to other nodes. In general, if all nodes have equal hardware capability, each one should have the same num_tokens value . The recommended value is 256. If this property is commented out (#num_tokens), Cassandra uses 1 (equivalent to #num_tokens : 1) for legacy compatibility and assigns tokens using the initial_token property.
# If this cluster is not using vnodes, comment out num_tokens or set num_tokens: 1 and use initial_token.
num_tokens: 256

# this is node-specific
# The listen_address in Cassandra's configuration specifies the IP address or hostname the node uses to connect to other nodes. Key considerations:
# Single-Node: For single-node setups, comment out this property or use the default (localhost).
# Multi-Node: In multi-node setups, set listen_address to the node's IP or hostname.
# Multi-Network: In complex setups with multiple networks or data centers, configure listen_address accordingly.
# Caution: Never set listen_address to 0.0.0.0; it's always incorrect.
# Exclusive Setting: Do not set both listen_address and listen_interface on the same node; choose one or the other.

# listen_address: 172.17.0.2

# This approach abstracts away the need to specify an IP address directly and simply tells Cassandra to use the network interface named eth0.
# It can be more convenient in situations where the network interface name (eth0) is stable and less likely to change, especially in static or well-configured environments.
# It's a way to delegate the responsibility of choosing the IP address to the system's network configuration.
# If your network setup is relatively stable and you want a more abstract way to specify the network interface without worrying about IP address changes, then using listen_interface can be more convenient.
# Note that this doesn't support ip-aliasing, IP aliasing is a technique used in computer networking to assign multiple IP addresses to a single network interface (NIC) on a computer or network device. This technique allows a single NIC to have multiple IP addresses associated with it, each serving different purposes or used for specific network-related tasks. 

listen_interface: eth0

# If an interface has an ipv4 and an ipv6 address, Cassandra uses the first ipv4 address by default. Set this property to true to configure Cassandra to use the first ipv6 address.
# since ipv6 is more improved version when compared to ipv4 hence set as true
listen_interface_prefer_ipv6: true

# while the commit log and CDC log directories should be within the same filesystem for management reasons, it's also a common practice to consider placing the commit log on a separate storage device or partition to improve write performance and data durability, especially in production environments with high write workloads.
# commit log.  when running on magnetic HDD, this should be a
# separate spindle than the data directories.
# If not set, the default directory is $CASSANDRA_HOME/data/commitlog.
commitlog_directory: /var/lib/cassandra/commitlog

# the data_file_directories setting is used to specify the directories where Cassandra stores its data files, including SSTables (Sorted String Table files). SSTables are the primary on-disk storage format for data in Cassandra.

# Here's an explanation of the data_file_directories setting:

# data_file_directories: This is a list of one or more directory paths where Cassandra will store its data files. You can specify multiple directories by separating them with commas. Cassandra will evenly distribute the data across these directories, helping to balance the I/O load.
# Having multiple data directories can help distribute the I/O load across multiple devices or disks, improving both read and write performance and increasing fault tolerance. It can also be useful when you have different types of storage devices (e.g., SSDs and HDDs) and want to allocate specific keyspaces or column families to different storage types based on their performance requirements.
# If not set, the default directory is $CASSANDRA_HOME/data/data.
data_file_directories: /var/lib/cassandra/data

# saved caches
# saved_caches_directory setting specifies the directory where Cassandra stores its saved caches. Saved caches are used to speed up certain read operations by caching frequently accessed data structures, such as the row key cache and the counter cache.
# The saved_caches_directory is used for storing various types of caches, including:

# Row Key Cache: This cache stores frequently accessed row keys, improving read performance by reducing the number of disk seeks when retrieving data.

# Counter Cache: The counter cache stores frequently accessed counter values (used in Cassandra's counter column type).

# Key Cache: The key cache stores frequently accessed partition keys, further optimizing read performance.

# Bloom Filter: While not exactly a cache, Cassandra's bloom filters are also persisted to disk in this directory. Bloom filters are used to reduce disk I/O by quickly determining whether data for a particular key may exist on disk.

# Having a dedicated directory for saved caches helps in managing cache data across node restarts and ensures that the cache data is persistent. This can lead to improved read performance, especially for frequently accessed data.

# It's important to allocate sufficient space for the saved_caches_directory, as the size of saved caches can grow over time depending on the workload and usage patterns. Additionally, you should monitor and maintain the saved cache directory to prevent it from consuming too much disk space.
# If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.
saved_caches_directory: /var/lib/cassandra/saved_caches

# determines how the system should respond when a commit disk failure occurs. A commit disk failure refers to a situation where the disk that stores the commit log, which is essential for ensuring data durability, encounters an issue

# policy for commit disk failures:
# die: shut down gossip and Thrift and kill the JVM, so the node can be replaced.
# stop: shut down gossip and Thrift, leaving the node effectively dead, but
#       can still be inspected via JMX.
# stop_commit: shutdown the commit log, letting writes collect but
#              continuing to service reads, as in pre-2.0.5 Cassandra
# ignore: ignore fatal errors and let the batches fail
# More conservative policies like "die" or "stop" prioritize data safety and require manual intervention to bring the node back online. Less conservative policies like "ignore" prioritize availability but can risk data loss or corruption.
commit_failure_policy : stop

# The strategy for optimizing disk read
# Possible values are:
# ssd (for solid state disks, the default)
# spinning (for spinning disks)
disk_optimization_strategy: ssd

# This setting defines the action Cassandra should take in response to a disk failure, specifically when one of the disks where data is stored becomes unavailable or fails.
# Disk failures can occur due to hardware issues, disk corruption, or other factors. When a disk failure is detected, Cassandra will respond according to the specified policy.
# policy for data disk failures:
# die: shut down gossip and client transports and kill the JVM for any fs errors or
#      single-sstable errors, so the node can be replaced.
# stop_paranoid: shut down gossip and client transports even for single-sstable errors,
#                kill the JVM for errors during startup.
# stop: shut down gossip and client transports, leaving the node effectively dead, but
#       can still be inspected via JMX, kill the JVM for errors during startup.
# best_effort: stop using the failed disk and respond to requests based on
#              remaining available sstables.  This means you WILL see obsolete
#              data at CL.ONE!
# ignore: ignore fatal errors and let requests fail, as in pre-1.2 Cassandra
disk_failure_policy: stop

# determines how Cassandra nodes perceive and locate each other within the cluster based on their network topology. The endpoint_snitch is a critical configuration option for cluster management, as it helps nodes make decisions about which other nodes to communicate with and how to route requests effectively.
# Cassandra provides several built-in endpoint snitches, each designed for different network topologies and use cases. Some common built-in snitches include:

# SimpleSnitch: A basic snitch that doesn't take network topology into account. It assumes that all nodes are in the same data center and rack.

# GossipingPropertyFileSnitch: This snitch uses a configuration file (cassandra-rackdc.properties) to specify the data center and rack assignments for each node. It's a good choice for small to medium-sized clusters with simple topologies.

# EC2Snitch: Designed for clusters deployed on Amazon EC2, this snitch uses EC2's metadata service to automatically determine the data center and rack information for nodes
 # PropertyFileSnitch
# Determines proximity by rack and datacenter, which are explicitly configured in cassandra-topology.properties file.
# Ec2MultiRegionSnitch
# Uses the public IP as the broadcast_address to allow cross-region connectivity. This means you must also set seed addresses to the public IP and open the storage_port or ssl_storage_port on the public IP firewall. For intra-region traffic, Cassandra switches to the private IP after establishing a connection.
# RackInferringSnitch:
# Proximity is determined by rack and datacenter, which are assumed to correspond to the 3rd and 2nd octet of each node's IP address, respectively. Best used as an example for writing a custom snitch class (unless this happens to match your deployment conventions).

# GoogleCloudSnitch:
# Use for Cassandra deployments on Google Cloud Platform across one or more regions. The region is treated as a datacenter and the availability zones are treated as racks within the datacenter. All communication occurs over private IP addresses within the same logical network.
# CloudstackSnitch
# Use the CloudstackSnitch for Apache Cloudstack environments.


endpoint_snitch: org.apache.cassandra.locator.GossipingPropertyFileSnitch
#  This setting is used for client communication with the Cassandra cluster. It defines the IP address or hostname that clients (such as application servers) should use to connect to the Cassandra cluster. The rpc_address is where the Cassandra Query Language (CQL) clients send their requests for reading or writing data.
# rpc_address : localhost


# The listen address for client connections. Interface must correspond to a single address, IP aliasing is not supported
rpc_interface: eth1


rpc_interface_prefer_ipv6: true

# In Apache Cassandra, a seed node is a special type of node within a Cassandra cluster that is used as an initial contact point for other nodes in the cluster to join or discover the cluster. Seed nodes play a crucial role in the node discovery process and are used during the bootstrapping of new nodes in the cluster. Here's what seed nodes are used for and how they work:

# Node Discovery: When a new node is added to a Cassandra cluster or when a node restarts, it needs to discover and establish initial connections with other nodes in the cluster to become a part of the cluster. Seed nodes act as the entry points for this discovery process.

# Gossip Protocol: Cassandra uses a protocol called the Gossip Protocol for node-to-node communication and cluster metadata distribution. Seed nodes are used to initiate the Gossip Protocol. When a new node starts, it contacts one or more seed nodes to get information about the cluster, such as the IP addresses of other nodes.

# Cluster Membership: Seed nodes maintain information about the cluster's topology and the addresses of other nodes. New nodes use seed nodes to learn about the existing nodes in the cluster and establish connections with them.

# Scalability: Seed nodes help maintain the scalability and stability of the cluster by providing a controlled way for new nodes to join. Without seed nodes, each node might need to know the addresses of all other nodes in the cluster, which can become impractical in large clusters.

# It's important to note that while seed nodes are used during the initial node discovery and cluster formation, they do not have any special status within the cluster once it is up and running. All nodes in a Cassandra cluster communicate with each other using the Gossip Protocol, and seed nodes are just the starting points for that communication.

# Typically, you would designate a few nodes in your Cassandra cluster as seed nodes by configuring them in the Cassandra configuration file (cassandra.yaml). These seed nodes should be stable and well-connected nodes within your cluster.

# In summary, seed nodes in Cassandra are initial contact points that help new or restarting nodes discover and join the cluster by providing information about the cluster's topology and other nodes.


seed_provider :

    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          # seeds is actually a comma-delimited list of addresses.
          # Ex: "<ip1>,<ip2>,<ip3>" 
          # this is node-specific
          - seeds: "172.17.0.2,172.17.0.3"

# UDFs (user defined functions) are disabled by default.
# As of Cassandra 3.0 there is a sandbox in place that should prevent execution of evil code.
enable_user_defined_functions: false

# Enables scripted UDFs (JavaScript UDFs).
# Java UDFs are always enabled, if enable_user_defined_functions is true.
# Enable this option to be able to use UDFs with "language javascript" or any custom JSR-223 provider.
# This option has no effect, if enable_user_defined_functions is false.
enable_scripted_user_defined_functions: false

# The setting is used to throttle the rate at which data compaction occurs. Compaction can be an I/O-intensive operation, and without rate limiting, it can potentially consume excessive system resources, causing performance issues.
# this determines the rate at which compaction processes are allowed to consume system resources, specifically measured in megabytes per second (MB/s). Compaction is the process of merging and compacting data files (SSTables) to optimize storage and improve read performance You can set compaction_throughput_mb_per_sec to a specific value in MB/s that represents the maximum rate at which compaction processes are allowed to run
# The optimal value for compaction_throughput_mb_per_sec depends on your hardware, workload, and the overall cluster configuration. You might need to experiment with different values to find the right balance. In general:

# For spinning hard drives (HDDs), a lower value is often appropriate, such as 16-64 MB/s, to avoid excessive I/O contention.
# For solid-state drives (SSDs) or high-performance storage, you can set a higher value, such as 128-256 MB/s, to take advantage of the faster I/O capabilities.
#  Setting the value to 0 disables compaction throttling.
compaction_throughput_mb_per_sec: 16

# Cassandra logs a warning when compacting partitions larger than the set value.
compaction_large_partition_warning_threshold_mb: 100

# The amount of on-heap memory allocated for memtables. Cassandra uses the total of this amount and the value of memtable_offheap_space_in_mb to set a threshold for automatic memtable flush.
memtable_heap_space_in_mb : 500
# Sets the total amount of off-heap memory allocated for memtables. Cassandra uses the total of this amount and the value of memtable_heap_space_in_mb to set a threshold for automatic memtable flush.
memtable_offheap_space_in_mb: 500

# Workloads with more data than can fit in memory encounter a bottleneck in fetching data from disk during reads. Setting concurrent_reads to (16 × number_of_drives) allows operations to queue low enough in the stack so that the OS and drives can reorder them. The default setting applies to both logical volume managed (LVM) and RAID drives
concurrent_reads: 32
# Writes in Cassandra are rarely I/O bound, so the ideal number of concurrent writes depends on the number of CPU cores on the node. The recommended value is 8 × number_of_cpu_cores
concurrent_writes: 32

#  A counter is a numeric value that can be incremented or decremented atomically across multiple columns within a single row. It's a data type used to keep track of counts or metrics in a distributed database like Cassandra.
#  This setting determines the number of simultaneous counter writes that Cassandra can handle concurrently on a single replica (node) without blocking other write operations. Counter writes read the current values before incrementing and writing them back. The recommended value is (16 × number_of_drives) .
concurrent_counter_writes: 32

# configuration file controls the level of parallelism for batch log writes. Batch log writes are used to ensure that multiple write operations (updates, inserts, deletes) are applied atomically, even if they affect multiple rows or columns across different tables.
# This setting determines the number of simultaneous batch log write operations that Cassandra can handle concurrently on a single replica (node) without blocking other write operations

concurrent_batchlog_writes: 32 

#  Limit on the number of concurrent materialized view writes. Set this to the lesser of concurrent reads or concurrent writes, because there is a read involved in each materialized view write.
concurrent_materialized_view_writes: 32

# Backs up data updated since the last snapshot was taken. When enabled, Cassandra creates a hard link to each SSTable flushed or streamed locally in a backups subdirectory of the keyspace data. Removing these links is the operator's responsibility.
incremental_backups: true

# Enables or disables taking a snapshot before each compaction. A snapshot is useful to back up data when there is a data format change. Be careful using this option: Cassandra does not clean up older snapshots automatically.
snapshot_before_compaction: true

# Adjusts the sensitivity of the failure detector on an exponential scale. Generally this setting does not need adjusting
phi_convict_threshold: 8

# commitlog_sync: periodic:

# When set to periodic, Cassandra will periodically sync (flush) data to the commit log based on the commitlog_sync_period_in_ms configuration parameter.
# The commitlog_sync_period_in_ms specifies the time interval in milliseconds between sync operations.
# This mode is less I/O-intensive than batch mode, as it doesn't immediately sync every write operation but rather batches them and syncs periodically. This can lead to a slight risk of data loss in case of a crash if data hasn't been synced yet.
# in this writes may be acked immediately
# and the CommitLog is simply synced every commitlog_sync_period_in_ms
# milliseconds. 
# commitlog_sync: periodic
# commitlog_sync_period_in_ms: 10000
# commitlog_sync: batch:

# When set to batch, Cassandra will immediately sync (flush) data to the commit log after each write operation. This means that every write operation is guaranteed to be safely written to the commit log before it is considered complete.
# batch mode provides stronger durability guarantees and minimizes the risk of data loss in case of a crash because data is immediately written to disk.
# However, it can be more I/O-intensive compared to periodic mode, potentially affecting write performance, especially on systems with high write throughput.
# commitlog_sync may be either "periodic" or "batch." 
# 
# When in batch mode, Cassandra won't ack writes until the commit log
# has been fsynced to disk.  It will wait
# commitlog_sync_batch_window_in_ms milliseconds between fsyncs.
# This window should be kept short because the writer threads will
# be unable to do extra work while waiting.  (You may need to increase
# concurrent_writes for the same reason.)
commitlog_sync: batch
commitlog_sync_batch_window_in_ms: 2

# controls the segment size on disk, where the commit logs are stored and once that size is reached then new segment is created.

commitlog_segment_size_in_mb: 32
