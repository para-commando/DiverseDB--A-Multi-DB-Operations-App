Mongodb:
This part of the project connects to Mongo Database using MongoDB Atlas which is a fully managed cloud database service provided by MongoDB, Inc. Typescript is the language used here. Mongoose was used to interact with the data as Object-Document Mapper library.

Mongoose is a Node.js ODM for MongoDB, enabling developers to define data schemas, enforce validation rules, and seamlessly interact with the database. With schema definition, Mongoose adds structure to MongoDB, facilitating data validation for types, defaults, and custom rules. The library's middleware support allows developers to execute custom logic before or after operations, and its powerful query API simplifies the construction of complex queries, enhancing the overall development experience. Additionally, Mongoose eases connection management, providing a connection pool and handling events related to database connections.

 In this CRUD Operations are performed where the operations are performed on the models created which are equivalent to tables in an RDBMS. Data is created using different methods such as creating an instance of each model and then calling .save() on the resulting object, using model.create().  

In the read operations model.aggregate(), .findOne(), findById(), .exists(), .populate() chained queries, along with the .virtual, .methods, .static, .query options defined on the model, are used.
 
In the update operations model.findOneAndUpdate, model.updateMany, modelUsed.updateOne are used.
 
In the delete operations model.deleteMany(), model.findOneAndDelete(), model.deleteOne(), model.findByIdAndDelete() are used.

Apache-cassandra:

This is written in vanilla javascript and for this a custom made docker image of cassandra db is used. The custom image is created using the official docker image present in docker hub provided by apache. the cassandra.yaml file which handles the configuration/setup of the apache-cassandra db servers is replaced by a custom modified file in the new custom image created using dockerfile. Main approach in this implementation used is bash scripting where different scripts are created for execution of each operations and all these scripts are finally run using an npm package called shelljs. Automation is brought into place by using scripts for almost all operations like creating or deleting the custom cassandra image, starting a cassandra cluster or entirely destroying them, checking their active status and many more. Then for each of CRUD operations CQL scripts are written and stored in corresponding .cql files then a script is run each of these .cql script files.

in the create script, CREATE KEYSPACE statements define two keyspaces, learn_cassandra_keyspaces and learn_cassandra_tables, with the latter employing the NetworkTopologyStrategy for replication. The CREATE TABLE statements showcase the creation of tables, such as my_table and users_by_country, specifying primary keys, data types, and additional settings like time-to-live and compaction strategy. Data insertion is demonstrated with the INSERT INTO statements, employing UUIDs and timestamps. Secondary indexes are created using the CREATE INDEX statements on the attributes column of the products table, enabling optimized querying. User-defined types, like address, are introduced, highlighting Cassandra's support for complex data structures. Additionally, the script features the creation of materialized views (cyclist_mat_view_rank_1) for efficient data retrieval and compound primary keys with clustering order in the calendarcalendarcalendar table. These options collectively illustrate the versatility and capabilities of Cassandra in managing distributed, scalable, and complex data models.

in the read script, various options and operations showcase the flexibility and capabilities of Cassandra for data retrieval and manipulation. The SELECT statements demonstrate basic querying, with conditions such as equality (WHERE country='US') and key containment (WHERE attributes CONTAINS KEY 'color'). Retrieving specific values within nested data is illustrated with queries like WHERE attributes['color'] = 'Red'. Write timestamps can be retrieved using WRITETIME on columns like 'category'. Aggregate functions, both built-in (total_price_agg) and user-defined (fifteenPctDiscountedPrice), highlight the capacity for custom calculations on data. Materialized views, such as cyclist_mat_view_rank_1, provide precomputed results for efficient querying. Time-to-live (TTL) functionality is demonstrated with SELECT TTL(cyclist_name), revealing the remaining time before data expiration. Limiting results per partition (PER PARTITION LIMIT) and filtering on non-indexed columns (ALLOW FILTERING) demonstrate strategies for managing query performance. These options collectively underline Cassandra's strengths in distributed, NoSQL database operations, offering a balance between flexibility and performance in handling diverse data models.

in the update script CQL statements involve alterations and updates to keyspaces and tables, demonstrating the dynamic nature of schema and data manipulation in Cassandra. The ALTER KEYSPACE command modifies the replication strategy of the learn_cassandra_keyspaces keyspace, adjusting its fault tolerance and availability characteristics. The UPDATE statements illustrate data modification in the rank_by_year_and_name and products tables. The use of USING TTL in some updates introduces time-to-live settings, allowing values to expire after a specified duration. Conditional updates (IF EXISTS and IF alternateNames[0]='Criterium du Dauphine') showcase the ability to perform updates based on specific conditions, ensuring data integrity. Additionally, the ALTER TABLE commands depict schema evolution by adding and dropping columns (new_users and new_users_2) in the my_table table. These operations collectively exemplify the flexibility and adaptability of Cassandra in accommodating changes to both schema and data in a distributed and scalable database environment.

in the delete script CQL statements involve the removal and truncation of keyspaces, tables, and specific data entries, showcasing the ability to manage and clean up data structures in a Cassandra database. The DROP KEYSPACE command removes the entire keyspace learn_cassandra_keyspaces, if it exists, along with all associated data and tables. The TRUNCATE statements clear all records from the my_table and users_by_country tables, while the subsequent DROP TABLE commands permanently delete these tables. Specific data deletions are illustrated using DELETE statements, removing entries from the products table's attributes and reviews columns based on specified conditions. Additionally, the DROP TYPE command removes the user-defined type address. The last two DELETE statements eliminate entries from the rank_by_year_and_name table, demonstrating the capability to selectively remove data based on complex conditions and predicates. These operations collectively exemplify Cassandra's robust data management and cleanup capabilities in a distributed and scalable NoSQL database environment.

MySQL:

in this part typescript is used as a language of choice and typeORM is used as an ORM to write queries for the db interaction. In this at first a datasource is initialized with all the necessary credentials. Then it is initialized before usage, entities are created as interface to a table in database for efficient interaction this entity comes variety of options to achieve desired results. This part also contains npm scripts to create migrations, entities, subscribers using cli along wit scripts to run the migrations. A table can easily be created by just creating an entity and then running the createEntityTable.js file under utils folder of useCases directory. In this the locally installed mysql db is used along with phpMyAdmin as interface for better interaction. Coming to the CRUD Operations, the create queries use queryBuilders to insert data into the database tables, Even raw queries are supported to be run. Coming to the read queries support a variety of operations such as all types of joins, locks, transaction, pagination, getOneOrFail, getMany, getOne and many more. The update operations are simple and can also be performed with insert queries as an alternative when exception occurs due to the prior existence of data and this generally called upsert operation. Coming to the delete operations there is an option of soft delete which marks the data as deleted along with the timestamps and this can also be recovered incase of restoration.


Neo4j:
In this part we are using an npm package called neo4j-driver to connect to an instance of neo4j database which is provided free of cost with a registered account in neo4j official website as they provide cloud hosted database service called auradb. To interact with the database we need to create a session using the neo4j-driver. That session provide multiple options to run the query and one of them is .run() where raw cypher queries can be run along with which there is an option called .executeWrite() which helps to run queries in a batch or transaction. In this Create operations are mainly performed using these 2 options where using session.run() nodes are created and using session.executeWrite() the relation are created between them. In this we are using a dataset which was custom made using a website called Mockaroo for our custom designed graph data model based on simple logistics company operations, later the data obtained is published on google sheets and its accessed remotely using its public link. This is required as we need to loan the dataset while creating nodes based on the data. Then in the update operations part the nodes and relationship are populated with the values using session.run() using some popular clause of cypher query language. In the read operation part multiple read queries are made using complex logic to navigate among the nodes for data retrieval and for this session.beginTransaction() is used which is used to run multiple queries back to back in a loop which returns an instance of .run()  until the the transaction is closed manually using .close(); on the instance returned by session.beginTransaction(), And in the delete operations part, entire graph data model which was created earlier is been deleted deleted.